{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43Ksofq7XHA2"
      },
      "source": [
        "#CHAPTER 11"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jrQnUADXKDx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Conv1D, GlobalMaxPooling1D, LSTM, GRU, Bidirectional, Dense, Flatten\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWyNYXN6XLeQ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "dataset, info = tfds.load('ag_news_subset', with_info=True, as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSaxSuFlXOCR"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad the sequences\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token=\"<OOV>\")\n",
        "train_texts = [x[0].numpy().decode('utf-8') for x in train_dataset]\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "sequences = pad_sequences(sequences, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esj8GGxWXQjp"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoding\n",
        "train_labels = [label.numpy() for _, label in train_dataset]\n",
        "train_labels = to_categorical(train_labels, num_classes=4)  # assuming 4 classes\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "train_sequences, val_sequences, train_labels, val_labels = train_test_split(sequences, train_labels, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-ToCGv12IRo"
      },
      "outputs": [],
      "source": [
        "vocab_size=20000\n",
        "embedding_dim =64\n",
        "max_length=sequences.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UmNOpqHjgdir"
      },
      "outputs": [],
      "source": [
        "# Define the DNN model\n",
        "model_dnn = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the CNN model\n",
        "model_cnn = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the LSTM model\n",
        "model_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(32, return_sequences=True),\n",
        "    LSTM(32),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "# Define the BiLSTM model\n",
        "model_BiLSTM = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(32, return_sequences=True)),\n",
        "    Bidirectional(LSTM(16)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # new layer\n",
        "    Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "models = [model_cnn, model_dnn, model_lstm, model_BiLSTM]\n",
        "\n",
        "for model in models:\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(train_sequences, train_labels, epochs=10, validation_data=(val_sequences, val_labels),verbose=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FUvf-CxIoqds",
        "outputId": "0df5c17c-7da6-4481-a30f-5040df4a0dbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "238/238 [==============================] - 1s 4ms/step - loss: 0.7756 - accuracy: 0.8989\n",
            "Model Evaluation - Model_CNN\n",
            "Loss: 0.7755934000015259\n",
            "Accuracy: 0.8989473581314087\n",
            "\n",
            "238/238 [==============================] - 1s 2ms/step - loss: 0.7091 - accuracy: 0.8896\n",
            "Model Evaluation - Model_DNN\n",
            "Loss: 0.7091193199157715\n",
            "Accuracy: 0.8896052837371826\n",
            "\n",
            "238/238 [==============================] - 2s 7ms/step - loss: 0.3211 - accuracy: 0.9008\n",
            "Model Evaluation - Model_LSTM\n",
            "Loss: 0.32113003730773926\n",
            "Accuracy: 0.9007894992828369\n",
            "\n",
            "238/238 [==============================] - 4s 10ms/step - loss: 0.5618 - accuracy: 0.8916\n",
            "Model Evaluation - Model_BiLSTM\n",
            "Loss: 0.5618014335632324\n",
            "Accuracy: 0.8915789723396301\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "test_texts = [x[0].numpy().decode('utf-8') for x in test_dataset]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
        "test_sequences = pad_sequences(test_sequences, padding='post', maxlen=sequences.shape[1])\n",
        "\n",
        "test_labels = [label.numpy() for _, label in test_dataset]\n",
        "test_labels = to_categorical(test_labels, num_classes=4)\n",
        "\n",
        "model_names = [\"Model_CNN\", \"Model_DNN\", \"Model_LSTM\", \"Model_BiLSTM\"]\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "  loss, accuracy = model.evaluate(test_sequences, test_labels)\n",
        "\n",
        "  print(\"Model Evaluation -\", model_names[i])\n",
        "  print(\"Loss:\", loss)\n",
        "  print(\"Accuracy:\", accuracy)\n",
        "  print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}